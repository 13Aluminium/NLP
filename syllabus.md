## Week 1: Introduction and Basic Text Processing
    •	Introduction to NLP: Definition and Application
	•	Text preprocessing techniques: Tokenization, Stemming (1), Lemmatization
	•	Removing stopwords, punctuation, and special characters
	•	Regular expressions for text processing
	•	N-grams and Bag-of-Words models
	•	Text normalization: Lowercasing, handling contractions
	•	Handling multilingual text and special characters

## Week 2: Spelling Correction, Language Modeling
    •	Levenshtein Distance and Edit Distance
    •	Noisy Channel Model for Correction
    •	N-gram language models (Unigram, Bigram, Trigram models)
    •	Smoothing techniques: Additive smoothing (Laplace), Good-Turing
    •	Perplexity and evaluating language models
    •	Applications of language models in NLP tasks (autocomplete, predictive typing)

## Week 3: Advanced Smoothing for Language Modeling, POS Tagging

    •	Advanced smoothing techniques: Katz Back-off, Kneser-Ney Smoothing
    •	Interpolated models
    •	Part-of-Speech (POS) tagging: Definition and importance
    •	POS tagging algorithms: Rule-based, HMM (Hidden Markov Model
    •	Viterbi algorithm for POS tagging
    •	Evaluation of POS tagging: Accuracy, Precision, Recall


## Week 4: Models for Sequential Tagging – MaxEnt, CRF
    •	Overview of sequential tagging tasks (POS, Chunking, NER)
    •	Maximum Entropy (MaxEnt) model for sequence tagging
    •	Feature engineering for MaxEnt model
    •	Conditional Random Fields (CRF): Introduction and formulation
    •	Difference between CRF and HMM
    •	Training CRF models for sequence tagging
    •	Applications of CRF in NLP (NER, POS tagging)


## Week 5: Syntax – Constituency Parsing
	•	Syntax in NLP: Importance and applications
	•	Phrase structure grammar: Context-free grammars (CFG)
	•	Parse trees: Constituency-based parse trees
	•	CKY algorithm for parsing
	•	Probabilistic CFG (PCFG): Probabilities in parsing
	•	Parsing algorithms: Earley Parser, CYK Algorithm
	•	Evaluating parsers: Precision, Recall, F1-score for parse trees


## Week 6: Dependency Parsing
	* 	Dependency grammar and its significance in NLP
	* 	Dependency trees and their structure
	* 	Transition-based vs. graph-based dependency parsing
	* 	Algorithms for dependency parsing: Eisner’s, Chu-Liu/Edmonds
	* 	Projective and non-projective parsing
	* 	Applications of dependency parsing in NLP (semantic role labeling, information extraction)	


## Week 7: Distributional Semantics	

	* Distributional hypothesis: “You shall know a word by the company it keeps”	
	* Vector space models for word representation
	* Word embeddings: Word2Vec, GloVe
	* Contextualized word representations: BERT, ELMo, GPT
	* Measuring word similarity and word analogies
	* Applications of distributional semantics: Word clustering, synonym detection

## Week 8: Lexical Semantics

	•	Lexical semantics: Definition and scope
	•	Word sense disambiguation (WSD): Knowledge-based and statistical methods
	•	Lexical relations: Synonymy, Antonymy, Hypernymy, Hyponymy
	•	Lexical databases: WordNet, BabelNet
	•	Polysemy and homonymy in NLP
	•	Applications of lexical semantics: Machine Translation, Information Retrieval


## Week 9: Topic Models

	•	Introduction to topic modeling: Definition and use cases
	•	Latent Dirichlet Allocation (LDA)
	•	Generative process of LDA
	•	Variational Inference and Gibbs Sampling for topic modeling
	•	Evaluating topic models: Perplexity, Coherence scores
	•	Applications of topic modeling: Document classification, Trend detection


## Week 10: Entity Linking, Information Extraction

	•	Named Entity Recognition (NER): Techniques and models (Rule-based, HMM, CRF)
	•	Entity linking: Mapping entities to knowledge bases (Wikipedia, DBpedia)
	•	Coreference resolution
	•	Relation extraction: Supervised and unsupervised methods
	•	Template-based information extraction
	•	Event extraction and knowledge graph construction


## Week 11: Text Summarization, Text Classification

	•	Extractive vs. Abstractive text summarization
	•	Techniques for extractive summarization: Frequency-based, TF-IDF, LexRank
	•	Abstractive summarization models: Seq2Seq, Transformer-based models
	•	Evaluation metrics for summarization: ROUGE, BLEU
	•	Supervised and unsupervised text classification
	•	Feature selection and engineering for text classification
	•	Common classifiers: Naive Bayes, SVM, Neural Networks


## Week 12: Sentiment Analysis and Opinion Mining

	•	Sentiment analysis: Lexicon-based vs. machine learning-based approaches
	•	Feature extraction for sentiment analysis: Sentiment lexicons, N-grams, embeddings
	•	Supervised models for sentiment analysis: SVM, Logistic Regression, LSTM
	•	Opinion mining: Subjectivity detection, opinion holder, opinion target
	•	Challenges in sentiment analysis: Sarcasm, domain adaptation
	•	Applications: Social media monitoring, product reviews, customer feedback
